\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{COMP-551 Kagle Competition\\
{\Large Team: Stochastic Gradient Descent into Madness}
}

\author{
\IEEEauthorblockN{Charles Alexandre Bedard}
\IEEEauthorblockA{\{email\}}
\IEEEauthorblockA{\{student id\}}
\and
\IEEEauthorblockN{Daniel Cimento}
\IEEEauthorblockA{daniel.cimento@mail.mcgill.ca}
\IEEEauthorblockA{260679318}
\and
\IEEEauthorblockN{Ziwen Wang}
\IEEEauthorblockA{\{email\}}
\IEEEauthorblockA{\{student id\}}
}

\maketitle

\section{Introduction}

For this project, we were tasked with training classifiers to recognize hand drawn images on a modified subset of the Google ``Quick, Draw!'' dataset.

This dataset was altered with noise artifacts and various transformations to the images, so we needed to keep those modifications in mind when designing our solution.

We tackled this project by trying a variety of image pre-processing methods and using three classifiers: an SVM with a linear kernel, a feed-forward neural network, and a convolutional neural network (CNN).

Of these, we saw the best performance with the CNN, and some aspects of image pre-processing helped produce the best results.

\section{Feature Design}

The biggest difficulties faced by our dataset were the noise and the number of features. The sizes of the images caused training to take a long time, and the noise interfered significantly with our linear model.

Our first attempt at pre-processing involved using smoothing and erosion based filters to reduce the impact of the noise and sharpen the main shape of the image under test. In doing this, we hoped our models would be more easily able to identify the features that were relevant to the label.

Our next approach was cropping the image based on pixel density, with the hope that the actual image would stand out from the noise. By cropping the image, we would drastically reduce the number of features to speed up training and improve performance by removing excessive noise interference.

This unfortunately did not perfectly crop the images in question, and it took a very long time to pre-process the images, so we revisited this approach by using a contour-based approach, isolating the largest contiguous region of pixels and cropping around it. This was much faster than the density approach, and it provided significantly cleaner results. In addition, since the crop size was based on the input data, we were confident that our resultant size would be as minimal as feasibly possible.

\section{Algorithms}

\subsection{Linear SVM}

Our ``baseline'' algorithm was an SVM with a linear kernel. Given the number of features and the interference with noise, we anticipated that the performance would be suboptimal, since finding key support vectors would be challenging.

We thought SVMs would be a better baseline model than alternatives like Decision Trees and Naive Bayes, as ultimately Decision Trees were too slow to train reliably on such a high dimensionality, and Naive Bayes has independence assumptions that are too strong, given the impact contiguity of pixels has on overall structure. 

Since Naive Bayes is a probabilistic approach, it may have had an advantage in handling the random noise, but ultimately we felt that the independence assumption outweighed that benefit, and we hoped to see similar improvements just from image preprocessing.

\subsection{Neural Network}

The neural network is considered a universal function approximator, which means that with enough nodes, a two-layer neural network can approximate any function to within a certain error bound. The function we are trying to emulate is one which, after training on the data, can output class labels for images depending on whether or not a given drawing appears in the image.

We firstly defined the feed forward neural network by creating multi-layered networks using the sigmoid function as our activation function. The network contains one input layer, one output layer, and a variable number of hidden layers. 

The structure of the network starts with the input layer, which contains values based directly on the input data. The input of the hidden layer nearest to the input layer will be the values of the input layer multiplied by weights established between the two layers. The weighted sum of the input layer will go through the activation function and become the output of the first hidden layer. By continuing this “forward-feeding” process, we can continue chaining layers together in this fashion until ultimately predicting a final output in output layer. This output will determine the label our network assigns to a data sample.

The training process of this is done through back-propagation: we calculate the partial derivative $\frac{\partial C}{\partial w}$ of the cost function $C$ with respect to any weight $w$ (which might be a bias, $b$) in the network and use that to update the weight for future iterations of training, until a satisfactory accuracy is reached or until our model stops improving. 

\subsection{Convolutional Neural Network}

A disadvantage of regular neural network is that it does not take into account the structure of the input data. As a result, the performance when classifying categories of data like images is poor. 
A better neural network method called Convolutional Neural Network has much better performance when classifying the image data. There are several reasons for this:

First, CNN takes a 3D tensor as input and its layer can transform an input 3D volume to output 3D with some differentiable function that may or may not have parameters. 

Second, the CNN uses local receptive fields, which allow neurons to only concern themselves with a small region of interest in the image. 

Lastly, the CNN uses parameter sharing and pooling. Parameter sharing allows us to treat certain local features the same, independent of where they are found in the image. Pooling allows us to place more emphasis on whether or not a feature occurs in an image, rather than the region where the feature occurs.

The hyper-parameters in this model include the batch size of each iteration, an optimizer, the learning rate of the optimizer, and the number of epochs. How we tuned these parameters will be discussed in the methodology section.

\section{Methodology}

\subsection{Linear SVM}

In implementing the SVM, we used Grid Search for cross-validation and hyperparameter tuning, testing values of the hyperparameters $C$ and $tol$ over a wide range (i.e. \{$10e-5, 10e-4, \ldots 10e5$\}).

\subsection{Neural Network}

%% TODO!!

\subsection{Convolutional Neural Network}

%% TODO!!!

\section{Results}

\subsection{Linear SVM}

For our SVM, we achieved predictably poor performance, with an accuracy of 4.366\%. Given that a hypothetical random classifier would be expected to have an accuracy of 3.22\%, this was an improvement, but not one significant enough to warrant much further exploration. From hyperparameter tuning, we didn't see any performance increase, and at more extreme hyperparameter values, we often failed to converge on our training set.

Erosion and smoothing based image pre-processing didn't show any improvement to the overall results, but cropping the images with the contour-based approach gave accuracies of 5.66\%, which was a relatively significant jump compared to our previous score.

\subsection{Neural Network}
%% TODO!!!
(Include results of neural network)

\subsection{Convolutional Neural Network}

Ultimately, our Convolutional Neural Network gave us the best performance recorded, with an accuracy of 69.966\%. However, when we applied smoothing and cropping algorithms to our input data, we found that our performance stayed the same, and in some cases decreased. So as a whole, we achieved our best results with unprocessed input data.

\subsection{Comparison}
%% TODO!!!
(Include any extra information, including a comparison between the three models)

\section{Discussion}

%%
%% FILLER TEXT
%% Remove and fill in the section's content
%%
Lorem ipsum dolor sit amet, consectetur adipiscing elit. Ut sit amet tincidunt est. Aliquam tellus augue, imperdiet non magna rhoncus, tincidunt placerat mi. Aliquam eleifend nulla finibus tortor tincidunt, non maximus lectus dictum. Donec nec ultrices orci, ut elementum felis. Suspendisse id aliquam orci. Aenean auctor, velit eu venenatis feugiat, arcu diam molestie felis, nec faucibus mauris orci vel ante. Phasellus scelerisque laoreet risus quis sagittis.

Cras feugiat, mauris sed maximus porta, enim elit molestie nulla, a imperdiet dui urna vel nisl. Etiam eu nulla augue. Nullam dui eros, fermentum ut viverra quis, venenatis nec lacus. Nullam congue eget ex eu vehicula. In sed lectus ac turpis lacinia suscipit non non ante. Phasellus lacus erat, sollicitudin et purus eget, venenatis euismod ante. Nulla vitae dui massa. Maecenas augue tortor, pharetra vitae blandit a, ultrices quis nibh. Sed facilisis enim libero, vitae pharetra nulla ornare at. Nam tristique enim vel ante interdum dictum. Quisque volutpat volutpat leo, in accumsan dui malesuada vel. Maecenas pellentesque augue in convallis tristique. Mauris elementum vel metus quis mattis. Nunc tristique scelerisque ullamcorper.

Quisque placerat est in enim maximus varius. Nunc nec lectus eu neque euismod viverra. Nullam non elementum purus, id pharetra sem. Vivamus vel vehicula tellus, eu pulvinar orci. Nunc scelerisque eget est sit amet luctus. Etiam et rhoncus odio, eu blandit eros. Phasellus gravida leo magna, quis ultricies risus viverra sit amet. Nunc finibus iaculis vehicula. Aenean nunc enim, sollicitudin at egestas ut, venenatis id ex. Proin at laoreet diam, ut viverra metus. Praesent a erat neque. Etiam volutpat lacinia dui interdum finibus. Praesent justo sem, scelerisque sit amet magna eu, maximus fermentum nibh.

Vivamus tempor pulvinar velit vitae venenatis. Etiam lectus massa, lacinia eu odio nec, iaculis congue tellus. Nam elit massa, malesuada quis diam ut, mollis rhoncus ex. Aliquam pellentesque lobortis lorem, interdum finibus elit ullamcorper vel. Nunc non hendrerit nibh, ut sagittis turpis. Donec dignissim lacus eu mi pretium tristique. Sed pharetra eu urna sed porttitor. Proin mollis turpis et nibh interdum ullamcorper. Nullam imperdiet fringilla lacus. Duis id nulla vitae lorem viverra hendrerit. Pellentesque id lorem eros. Duis semper blandit facilisis. Curabitur sed tempus enim.

Sed id pretium neque, sed pretium mi. Vestibulum eget tincidunt felis, id mollis dolor. Duis eu sollicitudin ligula, sed lobortis massa. Donec egestas convallis maximus. Mauris auctor egestas tellus nec vestibulum. Orci varius natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. In aliquet blandit elit a tempus. Donec urna augue, cursus quis leo a, porta blandit ligula. Quisque aliquet pellentesque nisl nec viverra. Maecenas dictum et lectus sed hendrerit. Nunc dapibus, nisl eu mattis hendrerit, eros lorem consectetur quam, ac accumsan erat risus vel metus. Quisque ipsum dui, bibendum sit amet suscipit non, pellentesque ac odio. Phasellus posuere, dui ut efficitur mattis, arcu arcu elementum ante, vel tristique mi mi sed nisl. Ut ut erat augue. Aliquam volutpat mollis vestibulum. 

\section{Statement of Contributions}

Charles made the most significant contributions to the methodology and implementation our pre-processing methods, handling the smoothing, erosion, and contour based approaches. He also contributed to the data analysis stemming from the pre-processing changes.

Daniel handled the linear SVM baseline, including hyperparameter tuning and the resulting data analysis. He also contributed to the methodology of the pre-processing methods and contributed to writing the report.

Ziwen took the initiative on the Neural Network and Convolutional Neural Network, handling the methodology, implementation, and data analysis.

We hereby state that all the work presented in this report is that of the authors. 

\end{document}