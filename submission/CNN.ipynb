{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "cdf51d9d78cc8ae3a511fe40f6ddcfbe2a60307c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import os\n",
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import sys\n",
    "stderr = sys.stderr\n",
    "sys.stderr = open(os.devnull, 'w')\n",
    "import keras\n",
    "sys.stderr = stderr\n",
    "from keras import *\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from keras.utils import plot_model\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "\n",
    "image_colmn = 100\n",
    "image_row  = 100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "a2a8fd8f5408687420d59c33da6ce725e4296c57"
   },
   "outputs": [],
   "source": [
    "def create_train_data():\n",
    "    class_label_name=[]\n",
    "    with open (train_label_filename,'r',) as csvfile:\n",
    "        readCSV = csv.reader(csvfile, delimiter=',')\n",
    "        for row in readCSV:\n",
    "            \n",
    "            class_label_name.append(row)\n",
    "    del class_label_name[0]\n",
    "    class_id_list = []\n",
    "    for i in range(len(class_name_list)):\n",
    "        class_id_list.append(i)\n",
    "        \n",
    "    hashmap = dict(zip(class_name_list,class_id_list))\n",
    "    class_label_vector=[]\n",
    "\n",
    "    for r in tqdm(class_label_name):\n",
    "        word = r[1]\n",
    "        if word in hashmap:\n",
    "            class_label_vector.append(hashmap[word])\n",
    "    images = np.load(read_train_filename,encoding='latin1')\n",
    "    train_data=[]\n",
    "    for i in tqdm(range(len(images))):\n",
    "            train_data.append([np.array((images[i][1]).reshape(1,10000)),np.array(class_label_vector[i])])\n",
    "    np.save('train_data.npy',train_data)\n",
    "    return train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "b9dd8ba7571235c80416904bd268c6206d673469"
   },
   "outputs": [],
   "source": [
    "def norm_input(x):\n",
    "    return (x - mean_px) / std_px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "b66c9e66579fbcfc4d73d2d135e9287c9c6d8dad"
   },
   "outputs": [],
   "source": [
    "read_train_filename = './all/train_images.npy'\n",
    "\n",
    "train_label_filename = './all/train_labels.csv'\n",
    "\n",
    "train_data_filename = './train_data.npy'\n",
    "class_name_list =[\"sink\",\"pear\",\"moustache\",\"nose\",\"skateboard\",\"penguin\",\"peanut\",\"skull\",\"panda\",\"paintbrush\",\"nail\",\"apple\",\"rifle\",\"mug\",\"sailboat\",\"pineapple\",\"spoon\",\"rabbit\",\"shovel\",\"rollerskates\",\"screwdriver\",\"scorpion\",\"rhinoceros\",\"pool\",\"octagon\",\"pillow\",\"parrot\",\"squiggle\",\"mouth\",\"empty\",\"pencil\"]\n",
    "num_classes = 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "f99f04169b9f81148b9656174d1602d1dbfc9c3b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exist\n"
     ]
    }
   ],
   "source": [
    "#checking processed data is exist.\n",
    "if (os.path.isfile(train_data_filename)==False):\n",
    "    train_data = create_train_data()\n",
    "        \n",
    "else:\n",
    "    print(\"file already exist\")\n",
    "    train_data = np.load(train_data_filename,encoding='latin1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "7f19d1b96cc8678e589ca2845b9d2964b8e1bf67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exist\n"
     ]
    }
   ],
   "source": [
    "read_test_filename = './all/test_images.npy'\n",
    "\n",
    "test_data_filename = './test_data.npy'\n",
    "\n",
    "if (os.path.isfile(test_data_filename)==False):\n",
    "    images = np.load(read_test_filename,encoding='latin1')\n",
    "    predict_data=[]\n",
    "    for i in tqdm(range(len(images))):\n",
    "            predict_data.append([np.array((images[i][1])).reshape(1,10000)])\n",
    "    np.save('test_data.npy',predict_data)\n",
    "    \n",
    "else:\n",
    "    print(\"file already exist\")\n",
    "    predict_data = np.load(test_data_filename,encoding='latin1')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "8ec0694a7d470de3a2e8f45ea8a213939909143b"
   },
   "outputs": [],
   "source": [
    "aug_epochs = 80\n",
    "test_size = 0.05\n",
    "#seed \n",
    "random_seed = 1\n",
    "learning_rate=0.001\n",
    "version =1\n",
    "batch_size=64\n",
    "epochs=50\n",
    "\n",
    "\n",
    "y_raw = [i[1] for i in train_data]\n",
    "#print(y_raw)\n",
    "#for y_e in y_raw:\n",
    "#    y.append(convert_y_to_vector(y_e))\n",
    "train_y=np.asarray(y_raw,dtype=float)\n",
    "\n",
    "#print(y)\n",
    "\n",
    "train_y = train_y.reshape((-1,1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "train_x = np.array([i[0] for i in train_data])\n",
    "train_x = train_x.reshape((-1,image_colmn,image_row))\n",
    "input_shape = (image_colmn, image_row, 1)\n",
    "predict_x = np.array([i[0] for i in predict_data])\n",
    "\n",
    "predict_x =predict_x.reshape((-1,image_colmn,image_row,1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "55daac15d6b5b21e85da8c2a63ed266763846154"
   },
   "outputs": [],
   "source": [
    "train_x = train_x.reshape((-1,image_colmn,image_row,1))\n",
    "train_y = train_y.reshape((train_y.shape[0],))\n",
    "train_x = train_x.astype('float32')\n",
    "predict_x = predict_x.astype('float32')\n",
    "train_x /= 255\n",
    "predict_x /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_uuid": "5dce16e122cef073a744600704c510d84c79c51e"
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(train_x, train_y, test_size=test_size, random_state=111)\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_uuid": "5621b213bfc19dbb8b3dd7825477c88bf22d67a1"
   },
   "outputs": [],
   "source": [
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "mean_px = x_train.mean().astype(np.float32)\n",
    "std_px = x_train.std().astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_uuid": "fa3561759f80bcd1cb54796d54cecb945495c4a8"
   },
   "outputs": [],
   "source": [
    "#https://keras.io/getting-started/sequential-model-guide/\n",
    "model = Sequential([\n",
    "        Lambda(norm_input, input_shape=input_shape),\n",
    "\n",
    "        Conv2D(32, strides=(1, 1),kernel_size=(3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        Conv2D(32, strides=(1, 1),kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2),strides=(1, 1)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        Flatten(),\n",
    "\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        # Dense(512, activation='relu'),\n",
    "        # BatchNormalization(),\n",
    "        # Dropout(0.5),\n",
    "        # Dense(512, activation='relu'),\n",
    "        # BatchNormalization(),\n",
    "        # Dropout(0.5),\n",
    "        Dense(31, activation='softmax')\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "3f485b73e0eb753f5cb1dd2e841c300e0867b142"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adam(lr=learning_rate),\n",
    "                  metrics=['accuracy'])   #https://keras.io/models/sequential/\n",
    "original = sys.stdout\n",
    "sys.stdout = open('history_' + str(version) + '.txt', 'w')\n",
    "\n",
    "model.summary()#https://keras.io/models/about-keras-models/#about-keras-models\n",
    "#https://keras.io/models/sequential/\n",
    "history = model.fit(x_train, y_train,\n",
    "                        batch_size=batch_size,\n",
    "                        epochs=epochs,\n",
    "                        verbose=1,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "7f1d6d71148577d2f8772fe81f168434dce13f5d"
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_uuid": "8234605043786ad0ec605e9a089902576250acac"
   },
   "outputs": [],
   "source": [
    "gen = ImageDataGenerator(rotation_range=15, width_shift_range=0.1, shear_range=0.3,\n",
    "                             height_shift_range=0.1, zoom_range=0.08)  # changed from 8, 0.08, 0.3, 0.08\n",
    "#https://keras.io/preprocessing/image/#imagedatagenerator-class\n",
    "\n",
    "\n",
    "batches = gen.flow(x_train, y_train, batch_size=batch_size)\n",
    "\n",
    "val_batches = gen.flow(x_test, y_test, batch_size=batch_size)\n",
    "\n",
    "model.fit_generator(batches, steps_per_epoch=x_train.shape[0] // batch_size, epochs=aug_epochs,\n",
    "                        validation_data=val_batches, validation_steps=x_test.shape[0] // batch_size,\n",
    "                        use_multiprocessing=False, class_weight=class_weights)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "sys.stdout = original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "e878dc38ce77724c80bda8af9530a64417a7aa99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model.to_json()\n",
    "json_name = \"model_\" + str(version) + \".json\"\n",
    "h5_name = \"model_\" + str(version) + \".h5\"\n",
    "with open(json_name, \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "model.save_weights(h5_name)\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_uuid": "508f88850b9027ae9cfcc4f8ecaf68f08d6a807e"
   },
   "outputs": [],
   "source": [
    "predict_result = model.predict_classes(predict_x, batch_size=batch_size, verbose=0)\n",
    "with open ('submission.csv','w',) as csvfile: \n",
    "    csvfile.write('Id')\n",
    "    csvfile.write(\",\")\n",
    "    csvfile.write('Category')\n",
    "    csvfile.write('\\n')\n",
    "    for i in range(len(predict_result)):\n",
    "            csvfile.write('%d'%i)\n",
    "            csvfile.write(\",\")\n",
    "            csvfile.write(class_name_list[predict_result[i]])\n",
    "            csvfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_uuid": "b9d5ad8a21a79d2267891d884b1c98843a9c6af0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.15207373 1.69310283 0.70937873 1.56352864 0.76804916 1.00475939\n",
      " 1.61290323 1.11843654 1.53225806 1.01474044 0.80222935 0.87060117\n",
      " 0.73313783 0.85362566 0.7585436  0.67649363 1.73136504 0.8856983\n",
      " 1.22580645 1.07526882 1.73136504 0.86813488 1.48044257 0.6825203\n",
      " 0.68711124 1.20177103 1.56352864 1.92736863 0.566454   1.00147586\n",
      " 1.27688172]\n"
     ]
    }
   ],
   "source": [
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "315d1b3d6432087cb54c4d79057e1645f82fd601"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "6966c30d9816b0beb36581f0b3bd6a65df6cdc47"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "041d8c981dfc674ef6a7079592680853f867e843"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "afc1877a541dd10035f0a429caef184ec2230181"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3d12650a360ac00aa3e17565fadbddad039ded58"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "680ddba1d80331f9bdf105632da2dad08757e5b8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c0f8ed3348f393e6aa707fd691dc2c21f2c21d55"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
