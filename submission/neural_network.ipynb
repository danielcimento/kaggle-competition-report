{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2 as cv\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "IMG_SIZE = 30\n",
    "\n",
    "train_label_filename = './train_labels.csv'\n",
    "train_images_filename = \"./scaled_train_images.npy\"\n",
    "test_images_filename = \"./scaled_test_images.npy\"\n",
    "formatted_data_filename = './formatted_train_data.npy'\n",
    "\n",
    "# Create a 'function' to convert labels to numbers\n",
    "class_name_list = [\n",
    "    \"sink\",\"pear\",\"moustache\",\n",
    "    \"nose\",\"skateboard\",\"penguin\",\n",
    "    \"peanut\",\"skull\",\"panda\",\n",
    "    \"paintbrush\",\"nail\",\"apple\",\n",
    "    \"rifle\",\"mug\",\"sailboat\",\n",
    "    \"pineapple\",\"spoon\",\"rabbit\",\n",
    "    \"shovel\",\"rollerskates\",\"screwdriver\",\n",
    "    \"scorpion\",\"rhinoceros\",\"pool\",\n",
    "    \"octagon\",\"pillow\",\"parrot\",\n",
    "    \"squiggle\",\"mouth\",\"empty\",\n",
    "    \"pencil\"]\n",
    "\n",
    "label_to_num = dict(zip(class_name_list, range(0, len(class_name_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "## Note: The following 3 cells are just for image pre-processing. \n",
    "## You can skip reading them if you've seen the pre-processing in another notebook\n",
    "##\n",
    "\n",
    "# Helper methods when working with arrays\n",
    "def imagify(origin_array, size):\n",
    "    #function to turn a 1d vector into a square matrix\n",
    "    #origin_aray -> any vector\n",
    "    #size -> the size of the matrix to create\n",
    "    new_array = np.zeros((size,size))\n",
    "    for i in range (0,size):\n",
    "        for j in range (0,size):\n",
    "            new_array[i][j] = origin_array[i*size+j]\n",
    "    return new_array\n",
    "\n",
    "def de_imagify(img, size):\n",
    "    #function to turn a square matrix into a vector\n",
    "    #img -> square matrix\n",
    "    #size -> the size of the square matrix\n",
    "    new_array = np.zeros((size ** 2))\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            new_array[i*size+j] = img[i][j]\n",
    "    return np.asarray(new_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_all_images(input_file_path, output_file_path):\n",
    "    #function to take all the images from the input_file_path, and crop them to a uniform size\n",
    "    #by default, all images are cropped and rescaled to 100,100\n",
    "    #the resulting images are saved into the output_file_path\n",
    "    #return: the size of the biggest cropped image, before rescaling\n",
    "    all_img = np.load(input_file_path, encoding='latin1')\n",
    "\n",
    "    #make an identical copy of the file, we will only modify the data of the images\n",
    "    cropped_img = all_img.copy()\n",
    "    #make a list to store the cropped images temporarily\n",
    "    cropped_list = []\n",
    "\n",
    "    #variables storing the size of the biggest image, used to resize all the samples\n",
    "    max_width = 0\n",
    "    max_height = 0\n",
    "    for i in tqdm(range(all_img.shape[0])):\n",
    "        #get the image in this row\n",
    "        img = imagify(all_img[i][1],100)\n",
    "        #make a copy that will remain unaltered\n",
    "        img_cpy = img.copy()\n",
    "        #blur the image\n",
    "        img = cv.GaussianBlur(img,(3,3),0)\n",
    "        # convert to grayscale\n",
    "        imgray = np.uint8(img * 255) \n",
    "        #convert to binary image\n",
    "        ret, thresh = cv.threshold(imgray, 20, 255, 0)\n",
    "        #get the contours in the image\n",
    "        im2, contours, hierarchy = cv.findContours(thresh, cv.RETR_TREE, cv.CHAIN_APPROX_SIMPLE)\n",
    "        #convert to rgb to have 3 channels\n",
    "        im2 = cv.cvtColor(im2, cv.COLOR_GRAY2RGB)   \n",
    "        #now get the biggest contour from the image\n",
    "        maxArea = 0\n",
    "        maxIndex = 0\n",
    "        if len(contours) != 0:\n",
    "            for i in range(len(contours)):\n",
    "                if cv.contourArea(contours[i]) > maxArea:\n",
    "                    maxArea = cv.contourArea(contours[i])\n",
    "                    maxIndex = i\n",
    "        #get the coordinates of the rectangle surrounding the shape\n",
    "        a,b,c,d = cv.boundingRect(contours[maxIndex])\n",
    "        #draw the rectangle\n",
    "        #cv.rectangle(img,(a,b),(a+c,b+d),(255),1)\n",
    "        #crop the original image\n",
    "        crop = img_cpy[b:b+d, a:a+c]\n",
    "        temp_max = np.max([c,d])\n",
    "        crop = cv.resize(crop,(100,100))\n",
    "        cropped_img[i][1] = de_imagify(crop,100)\n",
    "        cropped_list.append(crop)\n",
    "        if (c > max_width):\n",
    "            max_width = c\n",
    "        if (d > max_height):\n",
    "            max_height = d\n",
    "\n",
    "    #get the max size, i.e. biggest value between width and height\n",
    "    max_size = np.max([max_height,max_width])\n",
    "    for i in tqdm(range(cropped_img.shape[0])):\n",
    "    #for i in tqdm(range(2)):\n",
    "        #resize the array\n",
    "        np.resize(cropped_img[i][1],(max_size ** 2))\n",
    "        #cropped_img[i][1].resize(max_size)\n",
    "        img = cropped_list[i]\n",
    "        #crop the image to the max size, as a square\n",
    "        crop = cv.resize(img,(max_size,max_size))\n",
    "        #cropped_img[i][1] = de_imagify(crop,max_size)\n",
    "        cropped_img[i][1] = de_imagify(crop,max_size)\n",
    "        #crop = cv.resize(crop,(100,100))\n",
    "\n",
    "\n",
    "    np.save(output_file_path, cropped_img)\n",
    "    return max_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_all(input_file_path,output_file_path, current_size, size):\n",
    "    #function to resize all the images in a file\n",
    "    #takes the images in input_file_path and puts the resized ones in output_file_path\n",
    "    #current_size -> the current size of the square matrices representing the images\n",
    "    #size -> the wanted size of the square matrices\n",
    "    all_img = np.load(input_file_path, encoding='latin1')\n",
    "    print(all_img.shape[0])\n",
    "    all_copy = all_img.copy()\n",
    "    img_list = []\n",
    "    for i in tqdm(range(all_img.shape[0])):\n",
    "        img = imagify(all_img[i][1],current_size)\n",
    "        #resize the array\n",
    "        np.resize(all_img[i][1],(size ** 2))\n",
    "        resized_img = cv.resize(img,(size,size))\n",
    "        all_img[i][1] = de_imagify(resized_img,size)\n",
    "    np.save(output_file_path,all_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function for our activation function, with additional\n",
    "# option for getting the derivative (when calculating gradient)\n",
    "def sigmoid(x, deriv=False):\n",
    "    if deriv:\n",
    "        return (sigmoid(x) * (1 - sigmoid(x)))\n",
    "    else: \n",
    "        return (1.0 / (1.0 + np.exp(-x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def square_error(prediction, correct_answer, deriv=False):\n",
    "    if deriv:\n",
    "        return np.subtract(prediction, correct_answer.reshape(31, 1))\n",
    "    else:\n",
    "        difference = np.subtract(prediction, correct_answer)\n",
    "        return np.sum(np.square(difference))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read in our given data file and convert its structure into something easier to work with\n",
    "def create_train_data():\n",
    "    training_labels = []\n",
    "    with open(train_label_filename) as label_file:\n",
    "        # Skip the header line\n",
    "        next(label_file)\n",
    "        for line in label_file:\n",
    "            # take the text label, convert it to a number\n",
    "            text_label = line.split(\",\")[1].strip()\n",
    "            label = label_to_num[text_label]\n",
    "            training_labels.append(label)\n",
    "        \n",
    "    # Turns an integer value into a vector with only that index set to 1\n",
    "    # This is used to turn our label into something we can use in our cost\n",
    "    # function computation. (final_layer - convert_y_to_vector(label))\n",
    "    def val_to_vec(val, size=len(class_name_list)):\n",
    "        vec = np.zeros((size,))\n",
    "        vec[val] = 1\n",
    "        return vec\n",
    "        \n",
    "#     size = crop_all_images('./train_images.npy','./scaled_train_images.npy')\n",
    "#     resize_all('./scaled_train_images.npy','./scaled_train_images.npy',size,30)\n",
    "#     size = crop_all_images('./test_images.npy','./scaled_test_images.npy')\n",
    "#     resize_all('./scaled_test_images.npy','./scaled_test_images.npy',size,30)\n",
    "\n",
    "    training_images = np.load(train_images_filename, encoding='latin1')\n",
    "    # Turn rows of (id, image) to (normalized image, vectorized label of image)\n",
    "    for i in tqdm_notebook(range(training_images.shape[0])):\n",
    "        training_images[i] = np.array([\n",
    "            (training_images[i][1] / np.sum(training_images[i][1])).reshape((IMG_SIZE**2, 1)), # Normalize the images\n",
    "            val_to_vec(training_labels[i]) # Image label as vector\n",
    "        ])\n",
    "    np.save(formatted_data_filename, training_images)\n",
    "    return training_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Check if we've already reformatted our input data, or if we need to recalculate it\n",
    "    if not os.path.isfile(formatted_data_filename):\n",
    "        train_data = create_train_data()\n",
    "    else:\n",
    "        print(\"Using pre-formatted data file\")\n",
    "        train_data = np.load(formatted_data_filename, encoding='latin1')\n",
    "\n",
    "# Separates the training data back out into Xs and ys\n",
    "Xs, ys = np.hsplit(train_data, 2)\n",
    "Xs, ys = np.stack(Xs.flatten()), ys.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, num_layers, input_size, output_size):\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # First we calculate the size of each layer, evenly spaced over the range from input to output\n",
    "        layer_sizes = np.linspace(output_size, input_size, num=num_layers, dtype=int)[::-1]\n",
    "        \n",
    "#     def __init__(self, layer_sizes):\n",
    "#         self.num_layers = len(layer_sizes)\n",
    "    \n",
    "        # Each layer is an MxN matrix, where N is the size of the input vector and M is the size of the output vector\n",
    "        self.weights = [np.random.rand(layer_sizes[i + 1], layer_sizes[i]) for i in range(0, self.num_layers - 1)]\n",
    "        self.biases = [np.random.rand(layer_sizes[i + 1], 1) for i in range(0, self.num_layers - 1)]\n",
    "        \n",
    "        # Define our default cost and activation functions\n",
    "        self.activation_func = sigmoid\n",
    "        self.cost_func = square_error\n",
    "        \n",
    "    def tune_parameters(self, Xs, ys, learn_rate = 1.0):\n",
    "        # Initialize an empty gradient\n",
    "        bias_gradient = [np.zeros(bias.shape) for bias in self.biases]\n",
    "        weight_gradient = [np.zeros(weight.shape) for weight in self.weights]\n",
    "        for X, y in zip(Xs, ys):\n",
    "            # Back-propagate a data point...\n",
    "            bias_change, weight_change = self.backpropagate(X, y)\n",
    "            # Then update the overall gradient with the individual point's gradient\n",
    "            bias_gradient = [bg + bc for bg, bc in zip(bias_gradient, bias_change)]\n",
    "            weight_gradient = [wg + wc for wg, wc in zip(weight_gradient, weight_change)]\n",
    "        # Update the weights and biases by the average gradient scaled by the learning rate\n",
    "        self.weights = [old_weight - (learn_rate / len(Xs)) * wg for old_weight, wg in zip(self.weights, weight_gradient)]\n",
    "        self.biases = [old_bias - (learn_rate / len(ys)) * bg for old_bias, bg in zip(self.biases, bias_gradient)]\n",
    "        \n",
    "    # Get the gradient for the cost function for a single data point\n",
    "    def backpropagate(self, X, y):\n",
    "        bias_gradient = [np.zeros(bias.shape) for bias in self.biases]\n",
    "        weight_gradient = [np.zeros(weight.shape) for weight in self.weights]\n",
    "        # We start with out activation just being our input\n",
    "        current_activation = X\n",
    "        activation_history = [X]\n",
    "        z_history = []\n",
    "        # First, pass our input all the way through the network\n",
    "        for bias, weight in zip(self.biases, self.weights):\n",
    "            # We normalize our calculation (by dividing over the sum) so that all our sigmoid values don't become 1\n",
    "            z = np.dot(weight, current_activation) + bias\n",
    "            z = z / np.sum(z)\n",
    "            z_history.append(z)\n",
    "            current_activation = self.activation_func(z)\n",
    "            activation_history.append(current_activation)\n",
    "        # Calculate the gradient for the last layer\n",
    "        sig =  self.activation_func(z_history[-1], deriv=True)\n",
    "        cost = self.cost_func(activation_history[-1], y, deriv=True)\n",
    "        delta = cost * sig\n",
    "        bias_gradient[-1] = delta\n",
    "        weight_gradient[-1] = np.dot(delta, activation_history[-2].transpose())\n",
    "        # Then, moving back through the other layers, propagate the weight shifts\n",
    "        for layer in range(2, num_layers):\n",
    "            z = z_history[-layer]\n",
    "            delta = np.dot(self.weights[-layer + 1].transpose(), delta) * sigmoid(z, deriv=True)\n",
    "            bias_gradient[-layer] = delta\n",
    "            weight_gradient[-layer] = np.dot(delta, activation_history[-layer - 1].transpose())\n",
    "        return bias_gradient, weight_gradient\n",
    "    \n",
    "    # Returns the vector result of our network's prediction\n",
    "    def feed_forward(self, X):\n",
    "        activation = X\n",
    "        for weight, bias in zip(self.weights, self.biases):\n",
    "            z = np.dot(weight, activation) + bias\n",
    "            z = z / np.sum(z)\n",
    "            activation = self.activation_func(z)\n",
    "        return activation\n",
    "    \n",
    "    def accuracy(self, Xs, ys):\n",
    "        correct = 0\n",
    "        for X, y in zip(Xs, ys):\n",
    "            if self.predict(X) == np.argmax(y):\n",
    "                correct += 1\n",
    "        return correct / len(Xs)\n",
    "    \n",
    "    # Returns the label result of our network's prediction\n",
    "    def predict(self, X):\n",
    "        vec = self.feed_forward(X)\n",
    "        return np.argmax(vec.reshape(31,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train our neural network on each of the cross-validation folds\n",
    "kfold = KFold(5, False)\n",
    "num_layers = 3\n",
    "\n",
    "best_network = None\n",
    "best_mse = sys.float_info.max\n",
    "\n",
    "epochs = 40\n",
    "\n",
    "for training_indices, validation_indices in tqdm_notebook(kfold.split(Xs), desc=\"K-Folds\", total=5):\n",
    "    \n",
    "    # Get the current folds of training/validation data\n",
    "    train_x, train_y = Xs[training_indices], ys[training_indices]\n",
    "    valid_x, valid_y = Xs[validation_indices], ys[validation_indices]\n",
    "    \n",
    "    # Initialize a bunch of random layers (containing weights and biases)\n",
    "    network = NeuralNetwork(num_layers, train_x.shape[1], len(class_name_list))\n",
    "    \n",
    "    # Perform gradient descent on the layers\n",
    "    for i in tqdm_notebook(range(epochs), desc=\"Epochs\"):\n",
    "        # Shuffle all our data (zipping to preserve order)\n",
    "        train_data = list(zip(train_x, train_y))\n",
    "        random.shuffle(train_data)\n",
    "        # Split into chunks of size 100\n",
    "        train_chunks = [train_data[x:x+100] for x in range(0, len(train_data), 100)]\n",
    "        for train_chunk in tqdm_notebook(train_chunks, desc=\"Training Batches\"):\n",
    "            # Unpack our chunk and tune our network on that chunk\n",
    "            batch_Xs, batch_ys = zip(*train_chunk)\n",
    "            network.tune_parameters(batch_Xs, batch_ys)\n",
    "            \n",
    "    network_error = sum([square_error(network.feed_forward(X), y) for X, y in zip(valid_x, valid_y)]) / len(valid_x)\n",
    "    if network_error < best_mse:\n",
    "        best_network = network\n",
    "        best_mse = network_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = np.load(test_images_filename, encoding='latin1')\n",
    "_, test_Xs = np.hsplit(test_data, 2)\n",
    "test_Xs = np.stack(test_Xs.flatten())\n",
    "\n",
    "predictions = [best_network.predict(X) for X in test_Xs]\n",
    "\n",
    "with open ('submission.csv','w',) as outfile:\n",
    "    outfile.write('Id,Category\\n')\n",
    "    for (i, label) in enumerate(predictions):\n",
    "        outfile.write(f\"{i},{class_name_list[label]}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
