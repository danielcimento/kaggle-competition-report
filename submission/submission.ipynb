{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file already exist\n",
      "file already exist\n",
      "10000\n",
      "10000\n",
      "[15  1  0 ...  4 15 13]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import os\n",
    "from random import shuffle\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import sys\n",
    "stderr = sys.stderr\n",
    "sys.stderr = open(os.devnull, 'w')\n",
    "import keras\n",
    "sys.stderr = stderr\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import Dense, Dropout, Flatten, Lambda\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from scipy.ndimage.filters import gaussian_filter\n",
    "from scipy.ndimage.interpolation import map_coordinates\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "from keras.utils import plot_model\n",
    "from keras.models import load_model\n",
    "\n",
    "image_colmn = 100\n",
    "image_row  = 100\n",
    "read_train_filename = './all/train_images.npy'\n",
    "\n",
    "train_label_filename = './all/train_labels.csv'\n",
    "train_data_filename = './train_data.npy'\n",
    "\n",
    "read_test_filename = './all/test_images.npy'\n",
    "\n",
    "test_data_filename = './test_data.npy'\n",
    "class_name_list =[\"sink\",\"pear\",\"moustache\",\"nose\",\"skateboard\",\"penguin\",\"peanut\",\"skull\",\"panda\",\"paintbrush\",\"nail\",\"apple\",\"rifle\",\"mug\",\"sailboat\",\"pineapple\",\"spoon\",\"rabbit\",\"shovel\",\"rollerskates\",\"screwdriver\",\"scorpion\",\"rhinoceros\",\"pool\",\"octagon\",\"pillow\",\"parrot\",\"squiggle\",\"mouth\",\"empty\",\"pencil\"]\n",
    "num_classes = 31\n",
    "\n",
    "\n",
    "def create_train_data():\n",
    "    class_label_name=[]\n",
    "    with open (train_label_filename,'r',) as csvfile:\n",
    "        readCSV = csv.reader(csvfile, delimiter=',')\n",
    "        for row in readCSV:\n",
    "            \n",
    "            class_label_name.append(row)\n",
    "    del class_label_name[0]\n",
    "    class_id_list = []\n",
    "    for i in range(len(class_name_list)):\n",
    "        class_id_list.append(i)\n",
    "        \n",
    "    hashmap = dict(zip(class_name_list,class_id_list))\n",
    "    class_label_vector=[]\n",
    "\n",
    "    for r in tqdm(class_label_name):\n",
    "        word = r[1]\n",
    "        if word in hashmap:\n",
    "            class_label_vector.append(hashmap[word])\n",
    "    images = np.load(read_train_filename,encoding='latin1')\n",
    "    train_data=[]\n",
    "    for i in tqdm(range(len(images))):\n",
    "            train_data.append([np.array((images[i][1]).reshape(1,10000)),np.array(class_label_vector[i])])\n",
    "    np.save('train_data.npy',train_data)\n",
    "    return train_data\n",
    "\n",
    "\n",
    "    \n",
    "        \n",
    "def norm_input(x):\n",
    "    return (x - mean_px) / std_px\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if (os.path.isfile(train_data_filename)==False):\n",
    "        train_data = create_train_data()\n",
    "                \n",
    "    else:\n",
    "        print(\"file already exist\")\n",
    "        train_data = np.load(train_data_filename,encoding='latin1')\n",
    "\n",
    "        \n",
    "    if (os.path.isfile(test_data_filename)==False):\n",
    "        images = np.load(read_test_filename,encoding='latin1')\n",
    "        predict_data=[]\n",
    "        for i in tqdm(range(len(images))):\n",
    "            predict_data.append([np.array((images[i][1])).reshape(1,10000)])\n",
    "        np.save('test_data.npy',predict_data)\n",
    "    \n",
    "    else:\n",
    "        print(\"file already exist\")\n",
    "        predict_data = np.load(test_data_filename,encoding='latin1')\n",
    "        \n",
    "    aug_epochs = 80\n",
    "    test_size = 0.05\n",
    "    #seed \n",
    "    random_seed = 1\n",
    "    learning_rate=0.001\n",
    "    version =1\n",
    "    batch_size=64\n",
    "    epochs=50\n",
    "    \n",
    "    y_raw = [i[1] for i in train_data]\n",
    "#print(y_raw)\n",
    "#for y_e in y_raw:\n",
    "#    y.append(convert_y_to_vector(y_e))\n",
    "    train_y=np.asarray(y_raw,dtype=float)\n",
    "\n",
    "#print(y)\n",
    "\n",
    "    train_y = train_y.reshape((-1,1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    train_x = np.array([i[0] for i in train_data])\n",
    "    train_x = train_x.reshape((-1,image_colmn,image_row))\n",
    "    input_shape = (image_colmn, image_row, 1)\n",
    "    predict_x = np.array([i[0] for i in predict_data])\n",
    "    predict_x =predict_x.reshape((-1,image_colmn,image_row,1))\n",
    "    print(len(predict_x))\n",
    "\n",
    "    train_x = train_x.reshape((-1,image_colmn,image_row,1))\n",
    "    train_y = train_y.reshape((train_y.shape[0],))\n",
    "    train_x = train_x.astype('float32')\n",
    "    predict_x = predict_x.astype('float32')\n",
    "    train_x /= 255\n",
    "    predict_x /= 255\n",
    "\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(train_x, train_y, test_size=test_size, random_state=111)\n",
    "    class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=np.unique(y_train), y=y_train)\n",
    "\n",
    "    y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "    mean_px = x_train.mean().astype(np.float32)\n",
    "    std_px = x_train.std().astype(np.float32)\n",
    "\n",
    "    \n",
    "   #https://keras.io/getting-started/sequential-model-guide/\n",
    "    model = Sequential([\n",
    "        Lambda(norm_input, input_shape=input_shape),\n",
    "\n",
    "        Conv2D(32, strides=(1, 1),kernel_size=(3, 3), activation='relu', padding='same', input_shape=input_shape),\n",
    "        Conv2D(32, strides=(1, 1),kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2),strides=(1, 1)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(128, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "\n",
    "        Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        Conv2D(256, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Dropout(0.25),\n",
    "        Flatten(),\n",
    "\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        Dense(512, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.5),\n",
    "        # Dense(512, activation='relu'),\n",
    "        # BatchNormalization(),\n",
    "        # Dropout(0.5),\n",
    "        # Dense(512, activation='relu'),\n",
    "        # BatchNormalization(),\n",
    "        # Dropout(0.5),\n",
    "        Dense(31, activation='softmax')\n",
    "    ])\n",
    "    model.load_weights('model_1.h5')\n",
    "    print(len(predict_x))\n",
    "    predict_result = model.predict_classes(predict_x, batch_size=batch_size, verbose=0)\n",
    "    print(predict_result)\n",
    "    with open ('submission.csv','w',) as csvfile: \n",
    "        csvfile.write('Id')\n",
    "        csvfile.write(\",\")\n",
    "        csvfile.write('Category')\n",
    "        csvfile.write('\\n')\n",
    "        for i in range(len(predict_result)):\n",
    "            csvfile.write('%d'%i)\n",
    "            csvfile.write(\",\")\n",
    "            csvfile.write(class_name_list[predict_result[i]])\n",
    "            csvfile.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
